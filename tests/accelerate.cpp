// tests/accelerate.cpp
/**
 * @file accelerate.cpp
 * @brief solvers/accelerate åç«¯æµ‹è¯•å¥—ä»¶
 * @author M4yGem1ni
 * @date 2025-11-09
 * 
 * æµ‹è¯• Apple Accelerate åç«¯çš„çŸ©é˜µè¿ç®—åŠŸèƒ½
 */

#include "solvers/accelerate.hpp"
#include "utils/logger.hpp"
#include <xtensor/containers/xarray.hpp>
#include <xtensor/generators/xbuilder.hpp>
#include <xtensor/io/xio.hpp>
#include <xtensor/core/xmath.hpp>
#include <xtensor/views/xview.hpp>           // ğŸ”´ æ·»åŠ  view æ”¯æŒ
#include <xtensor/generators/xrandom.hpp>         // ğŸ”´ æ·»åŠ éšæœºæ•°æ”¯æŒ
#include <iostream>
#include <iomanip>
#include <cmath>

using namespace xt;
using namespace transformer;

/**
 * @brief è¾…åŠ©å‡½æ•°ï¼šå°† xtensor å¯¹è±¡è½¬æ¢ä¸ºå­—ç¬¦ä¸²
 */
template<typename E>
std::string xarray_to_string(const E& expr) {
    std::ostringstream oss;
    oss << expr;
    return oss.str();
}

// ============================================
// æµ‹è¯•è¾…åŠ©å‡½æ•°
// ============================================

/**
 * @brief æ£€æŸ¥ä¸¤ä¸ªæ•°ç»„æ˜¯å¦è¿‘ä¼¼ç›¸ç­‰
 */
template<typename T>
bool arrays_close(const xarray<T>& a, const xarray<T>& b, T tolerance = 1e-5) {
    if (a.shape() != b.shape()) {
        return false;
    }
    
    for (size_t i = 0; i < a.size(); ++i) {
        if (std::abs(a.data()[i] - b.data()[i]) > tolerance) {
            return false;
        }
    }
    return true;
}

/**
 * @brief æ‰“å°æµ‹è¯•ç»“æœ
 */
void print_test_result(const std::string& test_name, bool passed) {
    if (passed) {
        LOG_INFO("  âœ… {}", test_name);
    } else {
        LOG_ERROR("  âŒ {}", test_name);
    }
}

/**
 * @brief æ‰“å°æ•°ç»„å½¢çŠ¶
 */
template<typename T>
std::string shape_str(const xarray<T>& arr) {
    std::string result = "[";
    for (size_t i = 0; i < arr.dimension(); ++i) {
        result += std::to_string(arr.shape()[i]);
        if (i < arr.dimension() - 1) result += ", ";
    }
    result += "]";
    return result;
}

// ============================================
// æµ‹è¯•ç”¨ä¾‹
// ============================================

/**
 * @brief æµ‹è¯• 1: ç®€å• 2D çŸ©é˜µä¹˜æ³•
 */
bool test_simple_2d_matmul() {
    LOG_INFO("\n=== Test 1: Simple 2D Matrix Multiplication ===");
    
    // å‡†å¤‡æµ‹è¯•æ•°æ®
    xarray<float> A = {{1.0f, 2.0f}, 
                       {3.0f, 4.0f}};
    
    xarray<float> B = {{5.0f, 6.0f}, 
                       {7.0f, 8.0f}};
    
    // é¢„æœŸç»“æœ: [[1*5+2*7, 1*6+2*8], [3*5+4*7, 3*6+4*8]]
    //          = [[19, 22], [43, 50]]
    xarray<float> expected = {{19.0f, 22.0f}, 
                              {43.0f, 50.0f}};
    
    LOG_INFO("A = \n{}", xarray_to_string(A));
    LOG_INFO("B = \n{}", xarray_to_string(B));
    
    // æ‰§è¡ŒçŸ©é˜µä¹˜æ³•
    auto C = accelerate::Backend::matmul_2d(A, B);
    
    LOG_INFO("C = A @ B = \n{}", xarray_to_string(C));
    LOG_INFO("Expected = \n{}", xarray_to_string(expected));
    
    // éªŒè¯ç»“æœ
    bool passed = arrays_close(C, expected, 1e-5f);
    print_test_result("Simple 2D matmul", passed);
    
    return passed;
}

/**
 * @brief æµ‹è¯• 2: ä¸åŒç»´åº¦çš„ 2D çŸ©é˜µä¹˜æ³•
 */
bool test_rectangular_2d_matmul() {
    LOG_INFO("\n=== Test 2: Rectangular 2D Matrix Multiplication ===");
    
    // A: 3x2, B: 2x4 -> C: 3x4
    xarray<float> A = {{1.0f, 2.0f}, 
                       {3.0f, 4.0f}, 
                       {5.0f, 6.0f}};
    
    xarray<float> B = {{1.0f, 2.0f, 3.0f, 4.0f}, 
                       {5.0f, 6.0f, 7.0f, 8.0f}};
    
    LOG_INFO("A shape: {}", shape_str(A));
    LOG_INFO("B shape: {}", shape_str(B));
    
    auto C = accelerate::Backend::matmul_2d(A, B);
    
    LOG_INFO("C shape: {}", shape_str(C));
    LOG_INFO("C = \n{}", xarray_to_string(C));
    
    // éªŒè¯å½¢çŠ¶
    bool shape_correct = (C.shape()[0] == 3 && C.shape()[1] == 4);
    
    // éªŒè¯ç¬¬ä¸€ä¸ªå…ƒç´ : 1*1 + 2*5 = 11
    bool value_correct = std::abs(C(0, 0) - 11.0f) < 1e-5f;
    
    bool passed = shape_correct && value_correct;
    print_test_result("Rectangular 2D matmul", passed);
    
    return passed;
}

/**
 * @brief æµ‹è¯• 3: æ‰¹é‡çŸ©é˜µä¹˜æ³•ï¼ˆ3Dï¼‰
 */
bool test_3d_batch_matmul() {
    LOG_INFO("\n=== Test 3: 3D Batch Matrix Multiplication ===");
    
    // æ‰¹é‡å¤§å° 2, æ¯ä¸ªçŸ©é˜µ 2x2
    xarray<float> A = {{{1.0f, 2.0f}, 
                        {3.0f, 4.0f}},
                       {{5.0f, 6.0f}, 
                        {7.0f, 8.0f}}};
    
    xarray<float> B = {{{1.0f, 0.0f}, 
                        {0.0f, 1.0f}},  // å•ä½çŸ©é˜µ
                       {{2.0f, 0.0f}, 
                        {0.0f, 2.0f}}}; // 2å€å•ä½çŸ©é˜µ
    
    LOG_INFO("A shape: {}", shape_str(A));
    LOG_INFO("B shape: {}", shape_str(B));
    
    auto C = accelerate::Backend::batch_matmul(A, B);
    
    LOG_INFO("C shape: {}", shape_str(C));
    LOG_INFO("C[0] = \n{}", xarray_to_string(view(C, 0, all(), all())));
    LOG_INFO("C[1] = \n{}", xarray_to_string(view(C, 1, all(), all())));

    // ç¬¬ä¸€æ‰¹: A[0] @ I = A[0]
    xarray<float> expected_0 = {{1.0f, 2.0f}, {3.0f, 4.0f}};
    bool batch_0_correct = arrays_close(
        xarray<float>(view(C, 0, all(), all())), 
        expected_0, 
        1e-5f
    );
    
    // ç¬¬äºŒæ‰¹: A[1] @ 2I = 2*A[1]
    xarray<float> expected_1 = {{10.0f, 12.0f}, {14.0f, 16.0f}};
    bool batch_1_correct = arrays_close(
        xarray<float>(view(C, 1, all(), all())), 
        expected_1, 
        1e-5f
    );
    
    bool passed = batch_0_correct && batch_1_correct;
    print_test_result("3D batch matmul", passed);
    
    return passed;
}

/**
 * @brief æµ‹è¯• 4: 4D æ‰¹é‡çŸ©é˜µä¹˜æ³•ï¼ˆAttention åœºæ™¯ï¼‰
 */
bool test_4d_batch_matmul_attention() {
    LOG_INFO("\n=== Test 4: 4D Batch Matrix Multiplication (Attention) ===");
    
    // æ¨¡æ‹Ÿ Attention: Q @ K^T
    // Q: [batch=1, heads=2, seq_q=3, d_k=4]
    // K_T: [batch=1, heads=2, d_k=4, seq_k=3]
    // Result: [batch=1, heads=2, seq_q=3, seq_k=3]
    
    xarray<float> Q = zeros<float>({1, 2, 3, 4});
    xarray<float> K_T = zeros<float>({1, 2, 4, 3});
    
    // å¡«å……ä¸€äº›å€¼
    for (size_t i = 0; i < Q.size(); ++i) {
        Q.data()[i] = static_cast<float>(i % 10) / 10.0f;
    }
    
    for (size_t i = 0; i < K_T.size(); ++i) {
        K_T.data()[i] = static_cast<float>((i + 5) % 10) / 10.0f;
    }
    
    LOG_INFO("Q shape: {}", shape_str(Q));
    LOG_INFO("K_T shape: {}", shape_str(K_T));
    
    auto scores = accelerate::Backend::batch_matmul(Q, K_T);
    
    LOG_INFO("Scores shape: {}", shape_str(scores));
    
    // éªŒè¯å½¢çŠ¶
    bool shape_correct = (
        scores.shape()[0] == 1 && 
        scores.shape()[1] == 2 && 
        scores.shape()[2] == 3 && 
        scores.shape()[3] == 3
    );

    LOG_INFO("Scores[0,0] = \n{}", xarray_to_string(view(scores, 0, 0, all(), all())));

    bool passed = shape_correct;
    print_test_result("4D batch matmul (Attention)", passed);
    
    return passed;
}

/**
 * @brief æµ‹è¯• 5: å¹¿æ’­ï¼ˆæ‰¹é‡å¤§å°ä¸åŒ¹é…ï¼‰
 */
bool test_broadcast_batch_matmul() {
    LOG_INFO("\n=== Test 5: Broadcast Batch Matrix Multiplication ===");
    
    // A: [1, 2, 2] - æ‰¹é‡å¤§å° 1
    // B: [3, 2, 2] - æ‰¹é‡å¤§å° 3
    // Result: [3, 2, 2] - å¹¿æ’­ A åˆ°æ‰¹é‡å¤§å° 3
    
    xarray<float> A = {{{1.0f, 2.0f}, 
                        {3.0f, 4.0f}}};  // æ‰¹é‡ 1
    
    xarray<float> B = {{{1.0f, 0.0f}, 
                        {0.0f, 1.0f}},
                       {{2.0f, 0.0f}, 
                        {0.0f, 2.0f}},
                       {{3.0f, 0.0f}, 
                        {0.0f, 3.0f}}};  // æ‰¹é‡ 3
    
    LOG_INFO("A shape: {}", shape_str(A));
    LOG_INFO("B shape: {}", shape_str(B));
    
    auto C = accelerate::Backend::batch_matmul(A, B);
    
    LOG_INFO("C shape: {}", shape_str(C));
    
    // éªŒè¯å½¢çŠ¶
    bool shape_correct = (
        C.shape()[0] == 3 && 
        C.shape()[1] == 2 && 
        C.shape()[2] == 2
    );
    
    // éªŒè¯ç¬¬ä¸€æ‰¹: A @ I = A
    xarray<float> expected_0 = {{1.0f, 2.0f}, {3.0f, 4.0f}};
    bool batch_0_correct = arrays_close(
        xarray<float>(view(C, 0, all(), all())), 
        expected_0, 
        1e-5f
    );
    
    // éªŒè¯ç¬¬äºŒæ‰¹: A @ 2I = 2A
    xarray<float> expected_1 = {{2.0f, 4.0f}, {6.0f, 8.0f}};
    bool batch_1_correct = arrays_close(
        xarray<float>(view(C, 1, all(), all())), 
        expected_1, 
        1e-5f
    );
    
    bool passed = shape_correct && batch_0_correct && batch_1_correct;
    print_test_result("Broadcast batch matmul", passed);
    
    return passed;
}

/**
 * @brief æµ‹è¯• 6: matmul_auto è‡ªåŠ¨é€‰æ‹©
 */
bool test_matmul_auto() {
    LOG_INFO("\n=== Test 6: matmul_auto (Automatic Selection) ===");
    
    bool all_passed = true;
    
    // æµ‹è¯• 2D è‡ªåŠ¨é€‰æ‹©
    xarray<float> A_2d = {{1.0f, 2.0f}, {3.0f, 4.0f}};
    xarray<float> B_2d = {{5.0f, 6.0f}, {7.0f, 8.0f}};
    
    auto C_2d = accelerate::Backend::matmul_auto(A_2d, B_2d);
    LOG_INFO("2D auto: shape = {}", shape_str(C_2d));
    
    bool test_2d = (C_2d.dimension() == 2);
    print_test_result("matmul_auto for 2D", test_2d);
    all_passed &= test_2d;
    
    // æµ‹è¯• 3D è‡ªåŠ¨é€‰æ‹©
    xarray<float> A_3d = zeros<float>({2, 3, 4});
    xarray<float> B_3d = zeros<float>({2, 4, 5});
    
    auto C_3d = accelerate::Backend::matmul_auto(A_3d, B_3d);
    LOG_INFO("3D auto: shape = {}", shape_str(C_3d));
    
    bool test_3d = (
        C_3d.shape()[0] == 2 && 
        C_3d.shape()[1] == 3 && 
        C_3d.shape()[2] == 5
    );
    print_test_result("matmul_auto for 3D", test_3d);
    all_passed &= test_3d;
    
    // æµ‹è¯• 4D è‡ªåŠ¨é€‰æ‹©
    xarray<float> A_4d = zeros<float>({1, 8, 10, 64});
    xarray<float> B_4d = zeros<float>({1, 8, 64, 10});
    
    auto C_4d = accelerate::Backend::matmul_auto(A_4d, B_4d);
    LOG_INFO("4D auto: shape = {}", shape_str(C_4d));
    
    bool test_4d = (
        C_4d.shape()[0] == 1 && 
        C_4d.shape()[1] == 8 && 
        C_4d.shape()[2] == 10 && 
        C_4d.shape()[3] == 10
    );
    print_test_result("matmul_auto for 4D", test_4d);
    all_passed &= test_4d;
    
    return all_passed;
}

/**
 * @brief æµ‹è¯• 7: æ•°å€¼ç²¾åº¦æµ‹è¯•
 */
bool test_numerical_precision() {
    LOG_INFO("\n=== Test 7: Numerical Precision ===");
    
    // åˆ›å»ºä¸€ä¸ªå·²çŸ¥ç»“æœçš„çŸ©é˜µä¹˜æ³•
    xarray<float> A = {{0.1f, 0.2f, 0.3f}, 
                       {0.4f, 0.5f, 0.6f}};
    
    xarray<float> B = {{1.0f, 2.0f}, 
                       {3.0f, 4.0f}, 
                       {5.0f, 6.0f}};
    
    // æ‰‹åŠ¨è®¡ç®—é¢„æœŸç»“æœ
    // C[0,0] = 0.1*1 + 0.2*3 + 0.3*5 = 0.1 + 0.6 + 1.5 = 2.2
    // C[0,1] = 0.1*2 + 0.2*4 + 0.3*6 = 0.2 + 0.8 + 1.8 = 2.8
    // C[1,0] = 0.4*1 + 0.5*3 + 0.6*5 = 0.4 + 1.5 + 3.0 = 4.9
    // C[1,1] = 0.4*2 + 0.5*4 + 0.6*6 = 0.8 + 2.0 + 3.6 = 6.4
    
    xarray<float> expected = {{2.2f, 2.8f}, 
                              {4.9f, 6.4f}};
    
    auto C = accelerate::Backend::matmul_2d(A, B);

    LOG_INFO("C = \n{}", xarray_to_string(C));
    LOG_INFO("Expected = \n{}", xarray_to_string(expected));

    // æ£€æŸ¥æ¯ä¸ªå…ƒç´ çš„ç²¾åº¦
    bool all_close = true;
    float max_error = 0.0f;
    
    for (size_t i = 0; i < 2; ++i) {
        for (size_t j = 0; j < 2; ++j) {
            float error = std::abs(C(i, j) - expected(i, j));
            max_error = std::max(max_error, error);
            if (error > 1e-5f) {
                all_close = false;
                LOG_WARN("Element ({}, {}) error: {}", i, j, error);
            }
        }
    }
    
    LOG_INFO("Maximum error: {:.2e}", max_error);
    
    bool passed = all_close && (max_error < 1e-5f);
    print_test_result("Numerical precision", passed);
    
    return passed;
}

/**
 * @brief æµ‹è¯• 8: é”™è¯¯å¤„ç†
 */
bool test_error_handling() {
    LOG_INFO("\n=== Test 8: Error Handling ===");
    
    bool all_passed = true;
    
    // æµ‹è¯• 1: ç»´åº¦ä¸åŒ¹é…
    try {
        xarray<float> A = {{1.0f, 2.0f}, {3.0f, 4.0f}};  // 2x2
        xarray<float> B = {{1.0f, 2.0f, 3.0f}};          // 1x3
        
        auto C = accelerate::Backend::matmul_2d(A, B);  // åº”è¯¥æŠ›å‡ºå¼‚å¸¸
        
        LOG_ERROR("Should have thrown dimension mismatch exception!");
        all_passed = false;
    } catch (const std::invalid_argument& e) {
        LOG_INFO("Correctly caught dimension mismatch: {}", e.what());
        print_test_result("Dimension mismatch detection", true);
    }
    
    // æµ‹è¯• 2: é 2D è¾“å…¥åˆ° matmul_2d
    try {
        xarray<float> A = zeros<float>({2, 3, 4});  // 3D
        xarray<float> B = zeros<float>({4, 5});      // 2D
        
        auto C = accelerate::Backend::matmul_2d(A, B);  // åº”è¯¥æŠ›å‡ºå¼‚å¸¸
        
        LOG_ERROR("Should have thrown non-2D exception!");
        all_passed = false;
    } catch (const std::invalid_argument& e) {
        LOG_INFO("Correctly caught non-2D input: {}", e.what());
        print_test_result("Non-2D input detection", true);
    }
    
    // æµ‹è¯• 3: æ‰¹é‡ä¹˜æ³•ç»´åº¦ä¸åŒ¹é…
    try {
        xarray<float> A = zeros<float>({2, 3, 5});  // [..., 3, 5]
        xarray<float> B = zeros<float>({2, 4, 6});  // [..., 4, 6]
        
        auto C = accelerate::Backend::batch_matmul(A, B);  // åº”è¯¥æŠ›å‡ºå¼‚å¸¸
        
        LOG_ERROR("Should have thrown batch dimension mismatch exception!");
        all_passed = false;
    } catch (const std::invalid_argument& e) {
        LOG_INFO("Correctly caught batch dimension mismatch: {}", e.what());
        print_test_result("Batch dimension mismatch detection", true);
    }
    
    return all_passed;
}

/**
 * @brief æµ‹è¯• 9: æ€§èƒ½åŸºå‡†æµ‹è¯•
 */
bool test_performance_benchmark() {
    LOG_INFO("\n=== Test 9: Performance Benchmark ===");
    
    const size_t warmup_runs = 5;
    const size_t benchmark_runs = 20;
    
    // æµ‹è¯•ä¸åŒå¤§å°çš„çŸ©é˜µ
    std::vector<size_t> sizes = {64, 128, 256, 512};
    
    for (size_t size : sizes) {
        xarray<float> A = xt::random::randn<float>({size, size});
        xarray<float> B = xt::random::randn<float>({size, size});
        
        // é¢„çƒ­
        for (size_t i = 0; i < warmup_runs; ++i) {
            auto C = accelerate::Backend::matmul_2d(A, B);
        }
        
        // åŸºå‡†æµ‹è¯•
        auto start = std::chrono::high_resolution_clock::now();
        
        for (size_t i = 0; i < benchmark_runs; ++i) {
            auto C = accelerate::Backend::matmul_2d(A, B);
        }
        
        auto end = std::chrono::high_resolution_clock::now();
        auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end - start);
        
        double avg_time_ms = duration.count() / static_cast<double>(benchmark_runs) / 1000.0;
        double gflops = (2.0 * size * size * size) / (avg_time_ms * 1e6);
        
        LOG_INFO("Matrix size: {}x{}", size, size);
        LOG_INFO("  Average time: {:.3f} ms", avg_time_ms);
        LOG_INFO("  Performance: {:.2f} GFLOPS", gflops);
    }
    
    print_test_result("Performance benchmark completed", true);
    return true;
}

/**
 * @brief æµ‹è¯• 10: å¤§è§„æ¨¡æ‰¹é‡ä¹˜æ³•ï¼ˆçœŸå® Transformer åœºæ™¯ï¼‰
 */
bool test_transformer_realistic_scenario() {
    LOG_INFO("\n=== Test 10: Transformer Realistic Scenario ===");
    
    // æ¨¡æ‹ŸçœŸå®çš„ Transformer å‚æ•°
    const size_t batch_size = 2;
    const size_t num_heads = 8;
    const size_t seq_len = 128;
    const size_t d_k = 64;
    
    LOG_INFO("Simulating Transformer Attention:");
    LOG_INFO("  batch_size: {}", batch_size);
    LOG_INFO("  num_heads: {}", num_heads);
    LOG_INFO("  seq_len: {}", seq_len);
    LOG_INFO("  d_k: {}", d_k);
    
    // Q @ K^T
    xarray<float> Q = xt::random::randn<float>({batch_size, num_heads, seq_len, d_k});
    xarray<float> K_T = xt::random::randn<float>({batch_size, num_heads, d_k, seq_len});
    
    LOG_INFO("\nComputing Q @ K^T...");
    auto start = std::chrono::high_resolution_clock::now();
    
    auto scores = accelerate::Backend::batch_matmul(Q, K_T);
    
    auto end = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start);
    
    LOG_INFO("Scores shape: {}", shape_str(scores));
    LOG_INFO("Time: {} ms", duration.count());
    
    // éªŒè¯å½¢çŠ¶
    bool shape_correct = (
        scores.shape()[0] == batch_size &&
        scores.shape()[1] == num_heads &&
        scores.shape()[2] == seq_len &&
        scores.shape()[3] == seq_len
    );
    
    // Attention @ V
    xarray<float> V = xt::random::randn<float>({batch_size, num_heads, seq_len, d_k});
    
    LOG_INFO("\nComputing Attention @ V...");
    start = std::chrono::high_resolution_clock::now();
    
    auto output = accelerate::Backend::batch_matmul(scores, V);
    
    end = std::chrono::high_resolution_clock::now();
    duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start);
    
    LOG_INFO("Output shape: {}", shape_str(output));
    LOG_INFO("Time: {} ms", duration.count());
    
    // éªŒè¯è¾“å‡ºå½¢çŠ¶
    bool output_shape_correct = (
        output.shape()[0] == batch_size &&
        output.shape()[1] == num_heads &&
        output.shape()[2] == seq_len &&
        output.shape()[3] == d_k
    );
    
    bool passed = shape_correct && output_shape_correct;
    print_test_result("Transformer realistic scenario", passed);
    
    return passed;
}

// ============================================
// ä¸»å‡½æ•°
// ============================================

int main() {
    try {
        // åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
        Logger::instance().init(
            "xtensor_accelerate_test",
            true,
            "logs/xtensor_accelerate_test.log",
            1024 * 1024 * 10,
            3
        );
        
        Logger::instance().set_level(Logger::Level::INFO);
        
        LOG_INFO("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
        LOG_INFO("â•‘  xtensor_accelerate Backend Test Suite                   â•‘");
        LOG_INFO("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
        LOG_INFO("");
        LOG_INFO("Date: 2025-11-09 12:40:34 (UTC)");
        LOG_INFO("User: M4yGem1ni");
        LOG_INFO("Platform: Apple Accelerate Framework");
        LOG_INFO("");
        
        // è¿è¡Œæ‰€æœ‰æµ‹è¯•
        std::vector<std::pair<std::string, std::function<bool()>>> tests = {
            {"Simple 2D matmul", test_simple_2d_matmul},
            {"Rectangular 2D matmul", test_rectangular_2d_matmul},
            {"3D batch matmul", test_3d_batch_matmul},
            {"4D batch matmul (Attention)", test_4d_batch_matmul_attention},
            {"Broadcast batch matmul", test_broadcast_batch_matmul},
            {"matmul_auto", test_matmul_auto},
            {"Numerical precision", test_numerical_precision},
            {"Error handling", test_error_handling},
            {"Performance benchmark", test_performance_benchmark},
            {"Transformer realistic scenario", test_transformer_realistic_scenario}
        };
        
        int passed_count = 0;
        int total_count = tests.size();
        
        for (const auto& [name, test_func] : tests) {
            try {
                if (test_func()) {
                    passed_count++;
                }
            } catch (const std::exception& e) {
                LOG_ERROR("Test '{}' threw exception: {}", name, e.what());
            }
        }
        
        // æ‰“å°æµ‹è¯•æ€»ç»“
        LOG_INFO("");
        LOG_INFO("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
        LOG_INFO("â•‘  Test Summary                                             â•‘");
        LOG_INFO("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
        LOG_INFO("Total tests: {}", total_count);
        LOG_INFO("Passed: {}", passed_count);
        LOG_INFO("Failed: {}", total_count - passed_count);
        LOG_INFO("Success rate: {:.1f}%", 
                (100.0 * passed_count) / total_count);
        
        if (passed_count == total_count) {
            LOG_INFO("");
            LOG_INFO("ğŸ‰ All tests passed! xtensor_accelerate is working correctly!");
            Logger::instance().shutdown();
            return 0;
        } else {
            LOG_ERROR("");
            LOG_ERROR("âŒ Some tests failed. Please review the output above.");
            Logger::instance().shutdown();
            return 1;
        }
        
    } catch (const std::exception& e) {
        LOG_CRITICAL("Fatal error: {}", e.what());
        Logger::instance().shutdown();
        return 1;
    }
}